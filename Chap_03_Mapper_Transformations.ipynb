{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3969bb86-31c1-4889-b3d0-26f3612e9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4af9ad83-8f88-44c0-addf-2f0eff80f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa8b56a-d560-4693-96cb-8a1278191a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/18 00:30:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee29bb-450a-432d-8b9b-dc0b8ca5bf3e",
   "metadata": {},
   "source": [
    "![Mapper_Tranformation](img/mapper_transformations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93013777-b12d-4950-8942-51b657cac850",
   "metadata": {},
   "source": [
    "- lazy evaluation: transformation will not be evaluated util action is executed\n",
    "- help to optimize: Spark can look at DAG entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551b943-41ba-4ce7-9c41-6f18a11dfcaf",
   "metadata": {},
   "source": [
    "## 1. map() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa05d7b9-5d8d-4637-b4d0-7915e1f1a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function map in module pyspark.rdd:\n",
      "\n",
      "map(self, f, preservesPartitioning=False)\n",
      "    Return a new RDD by applying a function to each element of this RDD.\n",
      "    \n",
      "    >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      "    >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      "    [('a', 1), ('b', 1), ('c', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.RDD.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509909d-af1a-4896-b191-dee1cafaeb5c",
   "metadata": {},
   "source": [
    "### 1.1 RDD mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88abc4d4-61b9-4dc4-8b7a-826bd3d3ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea1f65d6-79c5-4ff0-a97a-95b7b615331e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1, -2, 3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, -1, -2, 3, 4]\n",
    "rdd_sample_2 = spark.sparkContext.parallelize(data)\n",
    "rdd_sample_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371addf9-3c96-4569-ac0e-da3f973de70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_relu = rdd_sample_2.map(lambda x : relu_func(x))\n",
    "rdd_relu.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52091169-518d-493b-a1c4-3c3bd0461369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or\n",
    "rdd_relu = rdd_sample_2.map(relu_func)\n",
    "rdd_relu.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b89faf-5800-4d8b-a38f-3ee68d333be4",
   "metadata": {},
   "source": [
    "- add new field to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6894673b-9ecc-468c-ba70-8342cb08140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', -1), ('d', -2), ('e', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [('a', 2), ('b', -1), ('d', -2), ('e', 3)]\n",
    "rdd_pairs = spark.sparkContext.parallelize(pairs)\n",
    "rdd_pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7594c212-0398-4bc6-83f6-e43863d80ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 2)), ('b', (-1, 0)), ('d', (-2, 0)), ('e', (3, 3))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_pairs_relu = rdd_pairs.map(lambda pair: (pair[0], (pair[1], relu_func(pair[1]) )))\n",
    "rdd_pairs_relu.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16f67b-5621-4c19-bd86-86e398cc2373",
   "metadata": {},
   "source": [
    "- more advance mapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c66ad4a9-0268-4bbe-89d2-26df7c449bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(record:str):\n",
    "    token = record.split(\",\")\n",
    "    name = token[1]\n",
    "    age = int(token[2])\n",
    "    num_friends = int(token[3])\n",
    "    return (name, (age, num_friends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6eed7b4-f547-498e-abac-5614ead5ebba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alex', (30, 124)),\n",
       " ('Bert', (32, 234)),\n",
       " ('Curt', (28, 312)),\n",
       " ('Don', (32, 180)),\n",
       " ('Mary', (30, 100)),\n",
       " ('Jane', (28, 212)),\n",
       " ('Joe', (28, 128)),\n",
       " ('Al', (40, 600))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_user = spark.sparkContext.textFile(\"data/users.txt\")\n",
    "rdd_pair = rdd_user.map(parse_record)\n",
    "rdd_pair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a808ea-e901-4032-bdbf-5c6ab9b5b562",
   "metadata": {},
   "source": [
    "### 1.2 DataFrame Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c451f0-d924-466e-b117-2573c9d12180",
   "metadata": {},
   "source": [
    "- rdd.map ~ use DataFrame.withColumn() then DataFrame.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b949fafc-3ae7-40b2-8295-88076219a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_user_degree = [ ('alex', 440, 'PHD'), ('jane', 420, 'PHD'),\n",
    "                    ('bob', 280, 'MS'), ('betty', 200, 'MS'),\n",
    "                    ('ted', 180, 'BS'), ('mary', 100, 'BS') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c813aac4-6422-426a-8ab6-0abce9394fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+\n",
      "| name|amount|education|\n",
      "+-----+------+---------+\n",
      "| alex|   440|      PHD|\n",
      "| jane|   420|      PHD|\n",
      "|  bob|   280|       MS|\n",
      "|betty|   200|       MS|\n",
      "|  ted|   180|       BS|\n",
      "| mary|   100|       BS|\n",
      "+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_degree = spark.createDataFrame(rdd_user_degree,\n",
    "                                       schema=[\"name\", \"amount\", \"education\"])\n",
    "df_user_degree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e90a3-59c0-43e3-aa9b-13c6a21e7fdc",
   "metadata": {},
   "source": [
    "#### 1.2.1 RDD approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a2b9b7a-106e-42bd-b7df-05d6afdeb681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|amount|education|bonus|\n",
      "+-----+------+---------+-----+\n",
      "| alex|   440|      PHD| 44.0|\n",
      "| jane|   420|      PHD| 42.0|\n",
      "|  bob|   280|       MS| 28.0|\n",
      "|betty|   200|       MS| 20.0|\n",
      "|  ted|   180|       BS| 18.0|\n",
      "| mary|   100|       BS| 10.0|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create bonus columns = 10% amount using RDD mapper\n",
    "df_bonus = (df_user_degree.rdd # convert to RDD\n",
    "            .map(lambda x: (x[\"name\"], x[\"amount\"], x[\"education\"], int(x[\"amount\"])/10))\n",
    "            .toDF([\"name\", \"amount\", \"education\", \"bonus\"])\n",
    "           )\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215108dd-d416-4045-855d-051624d13289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|amount|education|bonus|\n",
      "+-----+------+---------+-----+\n",
      "| alex|   440|      PHD| 44.0|\n",
      "| jane|   420|      PHD| 42.0|\n",
      "|  bob|   280|       MS| 28.0|\n",
      "|betty|   200|       MS| 20.0|\n",
      "|  ted|   180|       BS| 18.0|\n",
      "| mary|   100|       BS| 10.0|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bonus = (df_user_degree.rdd\n",
    "            .map(lambda x: x + (str(x[\"amount\"]/10),))\n",
    "            .toDF(df_user_degree.columns + [\"bonus\"])\n",
    "           )\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282f363-a4ea-4e9c-b03f-b3376bb2726e",
   "metadata": {},
   "source": [
    "#### 1.2.2 withColumn approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9ea630a-19b8-493d-bd35-12213d551dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|amount|education|bonus|\n",
      "+-----+------+---------+-----+\n",
      "| alex|   440|      PHD| 44.0|\n",
      "| jane|   420|      PHD| 42.0|\n",
      "|  bob|   280|       MS| 28.0|\n",
      "|betty|   200|       MS| 20.0|\n",
      "|  ted|   180|       BS| 18.0|\n",
      "| mary|   100|       BS| 10.0|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bonus = df_user_degree.withColumn(\"bonus\", F.lit(df_user_degree.amount/10))\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd448f5b-f132-49ad-987e-3bb08411f1ef",
   "metadata": {},
   "source": [
    "#### 1.2.3 Mapper multiple DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "257c8e7f-a8ad-4770-af8c-c1d4055e291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bonus(amount, education):\n",
    "    '''\n",
    "    Calculate bonus base on education\n",
    "    if PhD ration = 0.3, master=0.2 and BS=0.1 total amount\n",
    "    '''\n",
    "    if education == \"PHD\": return int(amount * 0.30)\n",
    "    if education == \"MS\": return int(amount * 0.20)\n",
    "    return int(amount * 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ace37d1-1dd5-408a-8f7a-6ab8ef13fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "609b8e2e-67fe-4375-ad55-eb8631e700ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_coumpute_bonus = F.udf(lambda amount, education:compute_bonus(amount, education),\n",
    "                          T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "246a1ba9-46da-4af7-a721-0122c101fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|amount|education|bonus|\n",
      "+-----+------+---------+-----+\n",
      "| alex|   440|      PHD|  132|\n",
      "| jane|   420|      PHD|  126|\n",
      "|  bob|   280|       MS|   56|\n",
      "|betty|   200|       MS|   40|\n",
      "|  ted|   180|       BS|   18|\n",
      "| mary|   100|       BS|   10|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bonus = (df_user_degree\n",
    "            .withColumn(\"bonus\",\n",
    "                        udf_coumpute_bonus(df_user_degree.amount, df_user_degree.education))\n",
    "           )\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f321a-1fee-4b8c-968e-898e28a805f7",
   "metadata": {},
   "source": [
    "## 2. flatMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e391e-498e-4092-a8a5-1e63f56a9d44",
   "metadata": {},
   "source": [
    "- make sure that the source RDD’s elements are iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b0e6a-f9cb-4420-9025-fdd7a4370fcd",
   "metadata": {},
   "source": [
    "### 2.1 Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6135a700-b9f0-4de9-bcd3-fbc1f521217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function flatMap in module pyspark.rdd:\n",
      "\n",
      "flatMap(self, f, preservesPartitioning=False)\n",
      "    Return a new RDD by first applying a function to all elements of this\n",
      "    RDD, and then flattening the results.\n",
      "    \n",
      "    >>> rdd = sc.parallelize([2, 3, 4])\n",
      "    >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      "    [1, 1, 1, 2, 2, 3]\n",
      "    >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      "    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.RDD.flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccf09f19-b6ec-49d6-aff0-01610a40d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"a\", \"red\", \"of\", \"fox\", \"jumped\"]\n",
    "rdd_words = spark.sparkContext.parallelize(words)\n",
    "rdd_words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b79b2d62-1533-4097-bc38-227ebbfab26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_flatmap_func(x):\n",
    "    if len(x) < 3:\n",
    "        return [x.lower(), x.lower()]\n",
    "    else:\n",
    "        return [x.upper(), x.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d9a75e6-eebd-47fe-ad58-4c591c5fc6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flattened = rdd_words.flatMap(my_flatmap_func)\n",
    "rdd_flattened.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7f04819-efb9-4097-920e-2234e1c48842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'RED', 'RED', 'of', 'of', 'FOX', 'FOX', 'JUMPED', 'JUMPED']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flattened.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efae6ef-38c8-41a4-963c-9535c74880e6",
   "metadata": {},
   "source": [
    "### 2.2 flatMap() with text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a4fcc09-fb75-4f69-b007-b238bcf18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b97c4354-e588-494d-8e41-225d3f5a7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_punctuation(record_str):\n",
    "    exclude = set(string.punctuation)\n",
    "    t = ''.join(ch for ch in record_str if ch not in exclude)\n",
    "    trimmed = re.sub('\\s+',' ', t)\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c3a7ef6-45b9-40f7-ad1e-5e19185bdb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fox, ran 2 fast!!!', 'Fox, jumped; of fence!!!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Fox, ran 2 fast!!!\", \"Fox, jumped; of fence!!!\"]\n",
    "rdd_sen = spark.sparkContext.parallelize(sentences)\n",
    "rdd_sen.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac881b19-a964-487c-ad80-bbe366447bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fox', 'ran', 'fast', 'Fox', 'jumped', 'fence']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cleaned = rdd_sen.map(no_punctuation)\n",
    "flattened = rdd_cleaned.flatMap(lambda v: v.split(\" \"))\n",
    "final_rdd = flattened.filter(lambda w: len(w) > 2)\n",
    "final_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d8bdd-aef8-4865-9de8-eebeab8ba3ba",
   "metadata": {},
   "source": [
    "### 2.3 Apply flatMap() to a DataFrame - F.explode(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cae43085-17c2-4171-a55d-10669b939e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmer = [('alex', ['Java','Scala', 'Python'], ['MS', 'PHD']),\n",
    "              ('jane', ['Cobol','Snobol'], ['BS', 'MS']),\n",
    "              ('bob', ['C++'], ['BS', 'MS', 'PHD']),\n",
    "              ('ted', ['Julia'], ['BS', 'MS']),\n",
    "              ('max', ['FORTRAN'], []),\n",
    "              ('dan', ['R'], [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82f4184c-7612-45a3-b223-5da78a208105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+-------------+\n",
      "|name|languages            |education    |\n",
      "+----+---------------------+-------------+\n",
      "|alex|[Java, Scala, Python]|[MS, PHD]    |\n",
      "|jane|[Cobol, Snobol]      |[BS, MS]     |\n",
      "|bob |[C++]                |[BS, MS, PHD]|\n",
      "|ted |[Julia]              |[BS, MS]     |\n",
      "|max |[FORTRAN]            |[]           |\n",
      "|dan |[R]                  |[]           |\n",
      "+----+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_programmer = spark.createDataFrame(data=programmer,\n",
    "                                      schema = ['name', 'languages', 'education'])\n",
    "df_programmer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4a9dbfa-1654-4277-bc64-eea9d4cfcf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+\n",
      "|name|language|education|\n",
      "+----+--------+---------+\n",
      "|alex|Java    |MS       |\n",
      "|alex|Java    |PHD      |\n",
      "|alex|Scala   |MS       |\n",
      "|alex|Scala   |PHD      |\n",
      "|alex|Python  |MS       |\n",
      "|alex|Python  |PHD      |\n",
      "|jane|Cobol   |BS       |\n",
      "|jane|Cobol   |MS       |\n",
      "|jane|Snobol  |BS       |\n",
      "|jane|Snobol  |MS       |\n",
      "|bob |C++     |BS       |\n",
      "|bob |C++     |MS       |\n",
      "|bob |C++     |PHD      |\n",
      "|ted |Julia   |BS       |\n",
      "|ted |Julia   |MS       |\n",
      "+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_programmer_exploded = (df_programmer\n",
    "                          .select(df_programmer.name,\n",
    "                                  F.explode(df_programmer.languages).alias('language'),\n",
    "                                  df_programmer.education\n",
    "                                 )\n",
    "                         )\n",
    "\n",
    "df_programmer_exploded = (df_programmer_exploded\n",
    "                          .select(df_programmer_exploded.name,\n",
    "                                  df_programmer_exploded.language,\n",
    "                                  F.explode(df_programmer_exploded.education).alias('education')\n",
    "                                 )\n",
    "                         )\n",
    "df_programmer_exploded.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d01d74-c2b0-480d-be3c-cad081079ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

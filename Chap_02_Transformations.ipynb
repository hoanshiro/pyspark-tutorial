{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f306fe72-a84c-4016-bb15-97d3c7c76d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/henry/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0132e5e1-c8ee-48de-95b1-3f9af42a95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb824be-3e19-4f29-80b3-a6d15dd337ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/17 22:43:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DNA\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a61958-4129-4077-9ba8-c33197b1d335",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4d6ce8-194f-4384-8ba1-28b140296933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">seq1\n",
      "cGTAaccaataaaaaaacaagcttaacctaattc\n",
      ">seq2\n",
      "agcttagTTTGGatctggccgggg\n",
      ">seq3\n",
      "gcggatttactcCCCCCAAAAANNaggggagagcccagataaatggagtctgtgcgtccaca\n",
      "gaattcgcacca\n",
      "AATAAAACCTCACCCAT\n",
      "agagcccagaatttactcCCC\n",
      ">seq4\n",
      "gcggatttactcaggggagagcccagGGataaatggagtctgtgcgtccaca\n",
      "gaattcgcacca\n"
     ]
    }
   ],
   "source": [
    "# view data\n",
    "!cat data/sample.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da35040c-2b02-4476-a3cc-b994706a37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_path = \"data/sample.fasta\"\n",
    "# call sparkContext.textFile to create rdd from file\n",
    "rdd_records = spark.sparkContext.textFile(records_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb0d3fb-33f9-4d0d-ae09-4e7fac911427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>seq1',\n",
       " 'cGTAaccaataaaaaaacaagcttaacctaattc',\n",
       " '>seq2',\n",
       " 'agcttagTTTGGatctggccgggg',\n",
       " '>seq3',\n",
       " 'gcggatttactcCCCCCAAAAANNaggggagagcccagataaatggagtctgtgcgtccaca',\n",
       " 'gaattcgcacca',\n",
       " 'AATAAAACCTCACCCAT',\n",
       " 'agagcccagaatttactcCCC',\n",
       " '>seq4',\n",
       " 'gcggatttactcaggggagagcccagGGataaatggagtctgtgcgtccaca',\n",
       " 'gaattcgcacca']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_records.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e8f27-49d7-4d9d-9968-f7afdd2a2f6a",
   "metadata": {},
   "source": [
    "## 2. DNA Base Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b434b2-8cf7-48e3-b156-00a23a9243be",
   "metadata": {},
   "source": [
    "### 2.1 Solution 1: User-defined function + reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fec978b-3821-4fdb-b24a-92c63fbd807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_records(line:str):\n",
    "    '''\n",
    "    Return list of tuple of (char, 1) with char='z' is number of sequence \n",
    "    and others are DNA.\n",
    "    All chars are lowercase\n",
    "    '''\n",
    "    key_value_list = []\n",
    "    if line.startswith(\">\"):\n",
    "        key_value_list.append((\"z\", 1))\n",
    "    else:\n",
    "        chars = line.lower()\n",
    "        for char in chars:\n",
    "            key_value_list.append((char, 1))\n",
    "    return key_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc321f1-bb17-4913-9b1e-aa9bc244263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bases = rdd_records.flatMap(process_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d0bbac-2990-4598-a62d-ac05a2c4ce47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('z', 1),\n",
       " ('c', 1),\n",
       " ('g', 1),\n",
       " ('t', 1),\n",
       " ('a', 1),\n",
       " ('a', 1),\n",
       " ('c', 1),\n",
       " ('c', 1),\n",
       " ('a', 1),\n",
       " ('a', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bases.take(10) # take 3 sample from rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeab15df-bbc8-4922-8814-0f7b0e27ec98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 61), ('g', 53), ('z', 4), ('t', 45), ('a', 73), ('n', 2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_frequencies = rdd_bases.reduceByKey(lambda x, y: x+y)\n",
    "rdd_frequencies.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "357b25e1-935e-4587-8983-e4ba86e3ad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 61, 'g': 53, 'z': 4, 't': 45, 'a': 73, 'n': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_frequencies.collectAsMap() # return as dict type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aa3542c-a294-4385-b823-9a6f3df267d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 61), ('g', 53), ('z', 4), ('t', 45), ('a', 73), ('n', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using groupBy\n",
    "grouped_rdd = rdd_bases.groupByKey()\n",
    "frequencies_rdd = grouped_rdd.mapValues(lambda values : sum(values))\n",
    "frequencies_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd39f04-a87d-4054-a9f1-bd60e468f383",
   "metadata": {},
   "source": [
    "1. Pros\n",
    "- easy to understand\n",
    "- no scalability issue\n",
    "2. Cons\n",
    "- cause memory problems if have too many (key, value)\n",
    "- high load on the network and prolong the shuffle time -> bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac257b83-426d-4f56-9da7-3712895f000e",
   "metadata": {},
   "source": [
    "### 2.2 Solution 2: Using HashMap (aka Dict) for processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "545d2a06-5632-4919-9205-d87622c801e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_records_hashmap(line:str):\n",
    "    '''\n",
    "    Return list of tuple of (char, 1) with char='z' is number of sequence \n",
    "    and others are DNA.\n",
    "    All chars are lowercase\n",
    "    '''\n",
    "    dict_dna = {}\n",
    "    if line.startswith(\">\"):\n",
    "        dict_dna[\"z\"] = 1\n",
    "    else:\n",
    "        chars = line.lower()\n",
    "        for char in chars:\n",
    "            if char in dict_dna:\n",
    "                dict_dna[char] += 1\n",
    "            else:\n",
    "                dict_dna[char] = 1\n",
    "    key_value_list = [(k, v) for k, v in dict_dna.items()]\n",
    "    return key_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879ea89c-27fe-4235-aa8f-674324539b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('z', 1), ('c', 8), ('g', 2), ('t', 7), ('a', 17)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_line_count = rdd_records.flatMap(process_records_hashmap)\n",
    "dna_line_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15604e39-ada7-46c2-94e7-1f8b64791f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 61), ('g', 53), ('z', 4), ('t', 45), ('a', 73), ('n', 2)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_count = dna_line_count.reduceByKey(lambda x, y: x + y)\n",
    "dna_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fb597-4076-49c2-8556-2fc184019a99",
   "metadata": {},
   "source": [
    "1. Pros:\n",
    "- Reduce number (key, value)\n",
    "2. Cons\n",
    "- still emitting up to six (key, value) pairs per DNA string line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efec0b-b51d-41aa-8d60-8d0cdbe99eb0",
   "metadata": {},
   "source": [
    "### 2.3 Solution 3: using mapPartions() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "253219ae-02dd-4e18-88f4-97fd2f14a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mapPartitions in module pyspark.rdd:\n",
      "\n",
      "mapPartitions(self, f, preservesPartitioning=False)\n",
      "    Return a new RDD by applying a function to each partition of this RDD.\n",
      "    \n",
      "    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "    >>> def f(iterator): yield sum(iterator)\n",
      "    >>> rdd.mapPartitions(f).collect()\n",
      "    [3, 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.RDD.mapPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d0f48-9d78-414b-b20e-d7ca5669892a",
   "metadata": {},
   "source": [
    "- map() is a 1-to-1 transformation\n",
    "- mapPartition() is a many-to-1 transformation by applying func() to each partition.\n",
    "- mapPartitions(func) transformation runs separately on each partition (block) of the RDD\n",
    "- func() must be of type iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75bc568d-32f3-4a6d-a2cd-83ae50d7c021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numOfPartitions = 3\n",
    "rdd_sample = spark.sparkContext.parallelize(numbers, numOfPartitions)\n",
    "rdd_sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4045241-1dc7-4cea-92e1-9130f0b9ba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sample.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0933702-f363-4da6-9b96-df688fcba915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "[7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "def scan(iterator):\n",
    "    print(list(iterator))\n",
    "rdd_sample.foreachPartition(scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ffcaf6d-7e2f-43d6-853b-171524c4de1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 15, 34]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adder(iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "rdd_sample.mapPartitions(adder).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26dd392c-3225-4382-81bb-8e5dc3c1f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_records_partition(iterator):\n",
    "    '''\n",
    "    Return list of tuple of (char, 1) with char='z' is number of sequence \n",
    "    and others are DNA for each partition\n",
    "    All chars are lowercase\n",
    "    '''\n",
    "    dict_dna = {\"z\": 0}\n",
    "    for line in iterator:\n",
    "        if line.startswith(\">\"):\n",
    "            dict_dna[\"z\"] += 1\n",
    "        else:\n",
    "            chars = line.lower()\n",
    "            for char in chars:\n",
    "                if char in dict_dna:\n",
    "                    dict_dna[char] += 1\n",
    "                else:\n",
    "                    dict_dna[char] = 1\n",
    "    key_value_list = [(k, v) for k, v in dict_dna.items()]\n",
    "    return key_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a1b509e-dccf-4977-90f5-6ec3533a905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_records.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34f46058-69fa-43dc-812e-9226b66061ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('z', 3),\n",
       " ('c', 28),\n",
       " ('g', 28),\n",
       " ('t', 24),\n",
       " ('a', 38),\n",
       " ('n', 2),\n",
       " ('z', 1),\n",
       " ('g', 25),\n",
       " ('a', 35),\n",
       " ('t', 21),\n",
       " ('c', 33)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_partition_count = rdd_records.mapPartitions(process_records_partition)\n",
    "dna_partition_count.collect() # only two 'z' keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f67b4bb3-0ce7-464c-b38c-a27e8e294ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 61), ('g', 53), ('z', 4), ('t', 45), ('a', 73), ('n', 2)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies_rdd = dna_partition_count.reduceByKey(lambda a, b: a + b)\n",
    "frequencies_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd2ace-a69d-4394-9578-11b597f4f948",
   "metadata": {},
   "source": [
    "1. Pros\n",
    "- not be a threat to scalability and will not use too much memory.\n",
    "2. Cons\n",
    "- require custom code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a399ab-b321-49e8-b702-491720955239",
   "metadata": {},
   "source": [
    "### 3. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54894558-3a30-40ac-8cf2-769f6123df42",
   "metadata": {},
   "source": [
    "- work on 'real' bigdata to test performance\n",
    "- use reduceByKey() instead of groupByKey()\n",
    "- use mapPartitions for reducing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633c813-2fe1-4415-bc79-6ddb41f69218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

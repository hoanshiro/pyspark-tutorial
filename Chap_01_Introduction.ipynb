{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534e1d7a-4b69-4e70-b42e-888363ffd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59238b98-9c63-4693-859a-9d80bb8e3dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/henry/spark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1523919-a6ab-4936-b05c-dd0908f524cd",
   "metadata": {},
   "source": [
    "- Apache Spark is considerably faster than Apache Hadoop because it uses in-memory caching and optimized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdb707a-a7a1-4cb7-853e-26049c689590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbd969-1934-48af-84dc-5720e27ab932",
   "metadata": {},
   "source": [
    "- Create an instance of this class, which gives you access to an instance of SparkContext\n",
    "- A SparkContext holds a connection to the Spark cluster manager and can be used to create RDDs and broadcast variables in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963b841f-e2ba-4910-b2bf-ecfcf44c08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/11 11:17:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create entry point to programing Spar\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f18477-d9df-4dcc-8df7-e8da80dd181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD from reading file\n",
    "records = spark.sparkContext.textFile(\"data/sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32dcf57-083e-4c57-abf9-758fffca05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Red,Fox,is,fast', 'red,Fox,by,Here']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing data\n",
    "records.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fed9b62-8624-4667-b773-28d4b9cf1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert each element of RDD records to lowercase\n",
    "lower_records = records.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85bce110-e59d-4f42-b350-bb54a7e292cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['red,fox,is,fast', 'red,fox,by,here']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_records.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28786548-edbe-4335-a7e9-42ed34dfec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-to-many transformation, convert each elements into a sequence of target elements\n",
    "# flatMap() apply function split(\",\") then flattening the result\n",
    "words = lower_records.flatMap(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "defcef3a-a8a0-4648-9cdd-d0def34839c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'fox', 'is', 'fast', 'red', 'fox', 'by', 'here']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3009f7af-593f-4e50-985a-de6802d3759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'fox', 'fast', 'red', 'fox', 'here']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using filter to select elements satisfy conditions\n",
    "filterd_words = words.filter(lambda word: len(word) > 2)\n",
    "filterd_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcdd5a-04a1-4dea-9ab5-22123600294a",
   "metadata": {},
   "source": [
    "Flow:\n",
    "- Turn data into Spark Abstraction(Data Structure)\n",
    "- Apply transformation and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ba780-10ab-472c-9f7f-9a96a1beb574",
   "metadata": {},
   "source": [
    "## 2. Web History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a02138-90c3-4857-8045-aa5d60538340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair(record):\n",
    "    '''\n",
    "    split <ulr_address>,<frequency>\n",
    "    '''\n",
    "    tokens = record.split(\",\")\n",
    "    url_address, frequency = tokens\n",
    "    return url_address, int(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2abb4f-ed6a-4e5e-9c2c-6fdcb4365eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def compute_stats(freq):\n",
    "    '''\n",
    "    take list of frequencies then return list of statistics\n",
    "    '''\n",
    "    avg = statistics.mean(freq)\n",
    "    median = statistics.median(freq)\n",
    "    sd = statistics.stdev(freq)\n",
    "    return avg, median, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a103a10-793f-4b45-9cb1-e6c45e0fb6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://mapreduce4hackers.com,19779',\n",
       " 'http://mapreduce4hackers.com,31230',\n",
       " 'http://mapreduce4hackers.com,15708',\n",
       " 'https://www.illumina.com,87000',\n",
       " 'https://www.illumina.com,58086']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = spark.sparkContext.textFile(\"data/web_history.txt\")\n",
    "history.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b4f24b-2fc0-48bc-ac1a-0f16058829c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://mapreduce4hackers.com,19779',\n",
       " 'http://mapreduce4hackers.com,31230',\n",
       " 'http://mapreduce4hackers.com,15708',\n",
       " 'https://www.illumina.com,87000',\n",
       " 'https://www.illumina.com,58086']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_his = history.filter(lambda record: len(record) > 5)\n",
    "valid_his.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050ab056-ea99-4825-a829-c438d57bdeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://mapreduce4hackers.com', 19779),\n",
       " ('http://mapreduce4hackers.com', 31230),\n",
       " ('http://mapreduce4hackers.com', 15708),\n",
       " ('https://www.illumina.com', 87000),\n",
       " ('https://www.illumina.com', 58086)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_his = valid_his.map(create_pair)\n",
    "pair_his.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c93d087c-9d1e-4b1b-9c65-dee07e9ebae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.illumina.com', (72543, 72543.0, 20445.285471227835)),\n",
       " ('http://mapreduce4hackers.com', (22239, 19779, 8048.094246466054))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_his = pair_his.groupByKey().mapValues(compute_stats)\n",
    "grouped_his.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677507a-dac9-4b30-ae97-afc46636350d",
   "metadata": {},
   "source": [
    "-> Make sure all functions work successly on single data before apply in Spark Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454a141-9a7b-4071-93b6-13590ea04039",
   "metadata": {},
   "source": [
    "## 3. RDD (resilient distributed dataset)\n",
    "RDDs support two types of operations\n",
    "- **transformations**, which transform the source RDD(s) into one or more new RDDs, \n",
    "- and **actions**,which transform the source RDD(s) into a non-RDD object such as a dictionary or array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd557d-51c1-4f47-90a4-c5d4df36a327",
   "metadata": {},
   "source": [
    "- transformation: map(), flatMap(), groupByKey(), reduceByKey(), filter()\n",
    "- RDDs are not evaluated until an action is performed on them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ad6ac-d2e6-4308-87fe-7ff09521d841",
   "metadata": {},
   "source": [
    "### 3.1 Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ffa711-0094-4c2a-bb94-3f9007feede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [('A', 7), ('A', 8), ('A', -4),\n",
    "         ('B', 3), ('B', 9), ('B', -1),\n",
    "         ('C', 1), ('C', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04a77301-c522-40da-95e6-7099b54efa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rdd = spark.sparkContext.parallelize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58be1af5-11ac-4f33-aaa0-c5fcfa0cb869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 8), ('B', 3), ('B', 9), ('C', 1), ('C', 5)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = sample_rdd.filter(lambda x: x[1] > 0)\n",
    "positive.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a28dfb91-dffd-4dca-b99d-7de5926f2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSumAndAvg(values):\n",
    "    total = sum(values)\n",
    "    avg = total/len(values)\n",
    "    return total, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab2690b9-7da6-4b17-9299-a401e959bd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 6.0)), ('C', (6, 3.0)), ('A', (15, 7.5))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues will apply on list of values after groupByKey finished\n",
    "sum_n_avg = positive.groupByKey().mapValues(lambda values: getSumAndAvg(values))\n",
    "sum_n_avg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36b07216-2188-47b7-8928-b9518d36c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (7, 1)),\n",
       " ('A', (8, 1)),\n",
       " ('B', (3, 1)),\n",
       " ('B', (9, 1)),\n",
       " ('C', (1, 1)),\n",
       " ('C', (5, 1))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative: using reduceByKey()\n",
    "positive_count = positive.mapValues(lambda v: (v, 1))\n",
    "positive_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f05b13-6fb4-4fc2-8eee-2a2e299b5362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 2)), ('C', (6, 2)), ('A', (15, 2))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_count = positive_count.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "sum_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd461ed-5aac-4ae7-83db-7f95bde36398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 6.0)), ('C', (6, 3.0)), ('A', (15, 7.5))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_n_avg = sum_count.mapValues(lambda x: (x[0], float(x[0])/x[1]))\n",
    "sum_n_avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a91fe5-b4a4-4939-bd95-40f993caa629",
   "metadata": {},
   "source": [
    "- groupByKey() can cause out of Memory error\n",
    "- reduceByKey data is combined in each partition -> only one output for each key send over the network\n",
    "- Overall, reduceByKey is more scaleable than groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710ed12-bde0-4818-bdb2-8d2d929a0bda",
   "metadata": {},
   "source": [
    "### 3.2 Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36a05c-fc83-4bfd-88ee-01f63cf6b894",
   "metadata": {},
   "source": [
    "1. Spark actions produces non-RDD values\n",
    "- reduce() -> single value\n",
    "- collect() -> list of datatype\n",
    "- count() -> number of elements of RDD\n",
    "- saveAsTextFile() -> save to disk\n",
    "- saveAsMap() -> save RDD\\[(K, V)\\] to disk as dict\\[K, V\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bf528-9fdc-447b-9e2d-b518f7681782",
   "metadata": {},
   "source": [
    "-> avoid using collect() to prevent memory error, instead using takeSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5941167c-6b35-43e4-99ac-ae30f6fd3333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 2)), ('C', (6, 2)), ('A', (15, 2))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_count.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1b69a-d2c1-455c-bd85-f3714bdcf9e8",
   "metadata": {},
   "source": [
    "## 4. DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6ea98-a3ab-44b7-bd67-eb4b87fef089",
   "metadata": {},
   "source": [
    "### 4.1 Read to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f66622be-b748-4c06-8b3c-9427b0322b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_path = \"data/infection.txt\"\n",
    "sdf_case = spark.read.load(virus_path, format=\"csv\",\n",
    "                           sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c68af284-8033-4b88-b850-27761f591432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+---------+\n",
      "|case_id|country|      city|infection_case|confirmed|\n",
      "+-------+-------+----------+--------------+---------+\n",
      "|  C0001|    USA|  New York|       contact|      175|\n",
      "|  C0008|    USA|New Jersey|       unknown|       25|\n",
      "|  C0009|    USA| Cupertino|       contact|      100|\n",
      "+-------+-------+----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_case.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26844ae-99ef-4b64-8dce-01ee74056f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_case.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32ece3-7b83-41d9-a41a-154f3a71d31c",
   "metadata": {},
   "source": [
    "### 4.2 Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1cf8cc3-ee80-4d0e-b382-9083e4755d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e947bb1a-020d-4965-bd67-39a49d8725af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+---------+\n",
      "|case_id|country|      city|infection_case|confirmed|\n",
      "+-------+-------+----------+--------------+---------+\n",
      "|  C0001|    USA|  New York|       contact|      175|\n",
      "|  C0009|    USA| Cupertino|       contact|      100|\n",
      "|  C0008|    USA|New Jersey|       unknown|       25|\n",
      "+-------+-------+----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_case.sort(F.desc(\"confirmed\")).show() # sort by confirmed column descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82679b52-e26b-4e92-a4ca-c2ff07dccc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+--------------+---------+\n",
      "|case_id|country|     city|infection_case|confirmed|\n",
      "+-------+-------+---------+--------------+---------+\n",
      "|  C0001|    USA| New York|       contact|      175|\n",
      "|  C0009|    USA|Cupertino|       contact|      100|\n",
      "+-------+-------+---------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_case.filter((sdf_case.confirmed > 50) & (sdf_case.country == \"USA\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde9155-73e0-4b24-90fb-f2dacd8c40e2",
   "metadata": {},
   "source": [
    "### 4.3 Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564a5b4-8ce4-4b52-8a5d-78adaff51342",
   "metadata": {},
   "source": [
    "- Create a DataFrame and find the average and sum of hours worked by employees per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "324c9c7b-c924-49bc-956d-fc31eec9f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_emps = [(\"Sales\", \"Barb\", 40), (\"Sales\", \"Dan\", 20),\n",
    "             (\"IT\", \"Alex\", 22), (\"IT\", \"Jane\", 24),\n",
    "             (\"HR\", \"Alex\", 20), (\"HR\", \"Mary\", 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64b8b210-2ca0-4db5-a3b2-0f920661a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_emps = spark.createDataFrame(dept_emps, schema=[\"dept\", \"name\", \"hours\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4af6bae1-19cc-4626-8c52-8d779fcff5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| dept|name|hours|\n",
      "+-----+----+-----+\n",
      "|Sales|Barb|   40|\n",
      "|Sales| Dan|   20|\n",
      "|   IT|Alex|   22|\n",
      "+-----+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_emps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9faa9320-6f2a-4ee5-a8de-8bf06f1f56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_avg_emps = sdf_emps.groupBy(\"dept\").agg(F.avg(\"hours\").alias(\"average\"),\n",
    "                           F.sum(\"hours\").alias(\"total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a85c1add-79ea-403f-99f8-4bbfd442ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| dept|average|total|\n",
      "+-----+-------+-----+\n",
      "|Sales|   30.0|   60|\n",
      "|   HR|   25.0|   50|\n",
      "|   IT|   23.0|   46|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_avg_emps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f1306-a46b-4f78-a764-7c1a9d1db1cd",
   "metadata": {},
   "source": [
    " - animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a3e4a6f-bf55-4a46-a4d1-70e673a7809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"fox\", 6), (\"dog\", 5), (\"fox\", 3), (\"dog\", 8),\n",
    "        (\"cat\", 1), (\"cat\", 2), (\"cat\", 3), (\"cat\", 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22c9028d-8ebc-4f71-a573-042a94deb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_animal = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59ff5d61-a5cd-4dfa-9ff8-580045397d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', 6), ('dog', 5), ('fox', 3)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_animal.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94aac275-04a1-4bdf-b588-f613d462fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_animal.count() # count number of animals in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3761249-69f0-4183-a3c6-5276047b49b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', 9), ('dog', 13), ('cat', 10)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sum = rdd_animal.reduceByKey(lambda x, y: x+y)\n",
    "rdd_sum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad32ce64-a338-4634-95f7-cadf6d8064a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', 9), ('cat', 10)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filterd_sum = rdd_sum.filter(lambda x: x[1] < 12)\n",
    "rdd_filterd_sum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aba73175-351a-44a9-b7e7-503c21f06396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', <pyspark.resultiterable.ResultIterable at 0x7f66f2a97d50>),\n",
       " ('dog', <pyspark.resultiterable.ResultIterable at 0x7f66f2a97d90>),\n",
       " ('cat', <pyspark.resultiterable.ResultIterable at 0x7f66f2a97850>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping Similar Keys in depth\n",
    "grouped = rdd_animal.groupByKey()\n",
    "grouped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15f5ca9f-db41-4ed9-841a-520d24242edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', [6, 3]), ('dog', [5, 8]), ('cat', [1, 2, 3, 4])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.map(lambda pair : (pair[0], list(pair[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d8359ba-37bc-4ba4-97db-e804880642fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', 9), ('dog', 13), ('cat', 10)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregating Values\n",
    "aggregated = grouped.mapValues(lambda values: sum(values))\n",
    "aggregated.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc0613-c851-4d5b-8b66-ee7b009e6bfd",
   "metadata": {},
   "source": [
    "## 5. ETL Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e3a16-6a7a-433b-b217-af698f41bb9b",
   "metadata": {},
   "source": [
    "- ETL is the general procedure of copying data from one or more sources into a destination system that represents the data differently from the source(s) or in a different context than the source(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f683b51-62ac-4208-a97e-6d067e17839e",
   "metadata": {},
   "source": [
    "*-* Let’s define our ETL process:\n",
    "1. Extraction\n",
    "- First, we create a DataFrame from a given JSON document.\n",
    "2. Transformation\n",
    "- Then we filter the data and keep the records for seniors (age > 54). Next, we add a new column, total, which is the total of males and females.\n",
    "3. Loading\n",
    "- Finally, we write the revised DataFrame into a MySQL database and verify the load process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fc7b42a-cc5e-4b7a-83c0-63f6a68d7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 data/census_2010.json\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/census_2010.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6422fa8c-9011-4980-aa8d-67e18d1e2af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"females\": 1994141, \"males\": 2085528, \"age\": 0, \"year\": 2010}\n",
      "{\"females\": 1997991, \"males\": 2087350, \"age\": 1, \"year\": 2010}\n",
      "{\"females\": 2000746, \"males\": 2088549, \"age\": 2, \"year\": 2010}\n",
      "{\"females\": 2002756, \"males\": 2089465, \"age\": 3, \"year\": 2010}\n",
      "{\"females\": 2004366, \"males\": 2090436, \"age\": 4, \"year\": 2010}\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/census_2010.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd04e0-023c-4fec-af9f-d62d1a2f68c5",
   "metadata": {},
   "source": [
    "### 5.1 Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb2ab5a3-731e-4cd1-aa77-4d4ade982b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_path = \"data/census_2010.json\"\n",
    "sdf_census = spark.read.json(census_path)\n",
    "sdf_census.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0c4b5fd-2182-4946-823c-8224edd784d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+----+\n",
      "|age|females|  males|year|\n",
      "+---+-------+-------+----+\n",
      "|  0|1994141|2085528|2010|\n",
      "|  1|1997991|2087350|2010|\n",
      "|  2|2000746|2088549|2010|\n",
      "+---+-------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_census.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12f640-6cc4-4be5-bde1-04ed11b78376",
   "metadata": {},
   "source": [
    "### 5.2 Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebbb710d-f568-445f-8b08-d818e5d12da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_seniors = sdf_census[sdf_census['age'] > 54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8164aa63-6239-4229-b3cd-f6e490b37a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_seniors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97be5bf0-32cb-4a2b-83d2-62c0ed1aec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+----+\n",
      "|age|females|  males|year|\n",
      "+---+-------+-------+----+\n",
      "| 55|2167706|2059204|2010|\n",
      "| 56|2106460|1989505|2010|\n",
      "| 57|2048896|1924113|2010|\n",
      "+---+-------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_seniors.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7e87c24-a795-4152-9d30-f3303f192285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_seniors_final = sdf_seniors.withColumn(\"total\",\n",
    "                                           F.lit(sdf_seniors.males + sdf_seniors.females))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c240ba63-8f64-411d-a0b6-2387a00ce860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+----+-------+\n",
      "|age|females|  males|year|  total|\n",
      "+---+-------+-------+----+-------+\n",
      "| 55|2167706|2059204|2010|4226910|\n",
      "| 56|2106460|1989505|2010|4095965|\n",
      "| 57|2048896|1924113|2010|3973009|\n",
      "+---+-------+-------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_seniors_final.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d253235-71f5-40e1-bd80-92487e3746c8",
   "metadata": {},
   "source": [
    "### 5.3 Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b28f7f-7c3e-4e95-a714-6d260e1c5f9e",
   "metadata": {},
   "source": [
    "- write the seniors_final DataFrame into a MySQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b14fc-5ba5-4773-9c35-b40c05d7a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_seniors_final\\\n",
    "# .write\\\n",
    "# .format(\"jdbc\")\\\n",
    "# .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "# .mode(\"overwrite\")\\\n",
    "# .option(\"url\", \"jdbc:mysql://localhost/testdb\")\\\n",
    "# .option(\"dbtable\", \"seniors\")\\\n",
    "# .option(\"user\", \"root\")\\\n",
    "# .option(\"password\", \"root_password\")\\\n",
    "# .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06a53f-d6f4-411a-aa89-a1ca3e1f8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ mysql -uroot -p\n",
    "# Enter password: <password>\n",
    "# Your MySQL connection id is 9\n",
    "# Server version: 5.7.30 MySQL Community Server (GPL)\n",
    "# mysql> use testdb;\n",
    "# Database changed\n",
    "# mysql> select * from seniors;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
